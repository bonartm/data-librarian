[
{
	"uri": "https://bonartm.github.io/data-librarian/cheatsheet/",
	"title": "Cheat Sheet",
	"tags": [],
	"description": "",
	"content": "Other Resources  https://librarycarpentry.org/ https://automatetheboringstuff.com/ https://librarycarpentry.org/lc-python-intro/  read in .csv files import pandas as pd file_path = \u0026#39;./Library_Usage.csv\u0026#39; df = pd.read_csv(file_path) show column names and their types df.columns df.dtypes number of rows len(df) summarize functions df.head() df.tail() df.describe() filter on columns df[[\u0026#34;colA\u0026#34;, \u0026#34;colB\u0026#34;, \u0026#34;colC\u0026#34;]] filter on rows by index df.iloc[[1, 10, 15]] filter on rows by logical query df.loc[df[\u0026#34;colA\u0026#34;] \u0026gt; 100] some descriptive statistics for columns df[\u0026quot;colA\u0026quot;].max() df[\u0026quot;colA\u0026quot;].max() df[\u0026quot;colA\u0026quot;].mean() df[\u0026quot;colA\u0026quot;].median() df[\u0026quot;colA\u0026quot;].std() df[\u0026quot;colA\u0026quot;].var() correlation df[\u0026#34;colA\u0026#34;].corr(df[\u0026#34;colB\u0026#34;], method=\u0026#39;pearson\u0026#39;) frequency tables df[\u0026#34;colA\u0026#34;].value_counts(normalize=True) cross tabulation pd.crosstab(df[\u0026#34;colA\u0026#34;], df[\u0026#34;colB\u0026#34;]) basic plots # histogramm df[\u0026#34;colA\u0026#34;].hist() # boxplot df[\u0026#34;colA\u0026#34;].plot.box() df.plot.box() # scatter plot df.plot.scatter(x=\u0026#39;colA\u0026#39;, y=\u0026#39;colB\u0026#39;) t-test "
},
{
	"uri": "https://bonartm.github.io/data-librarian/descriptive_statistics/univariate/frequency/",
	"title": "Häufigkeiten",
	"tags": [],
	"description": "",
	"content": "Kategoriale (nominale und ordinale) Variablen werden in Häufigkeitstabellen zusammengefasst. Dabei wird für jede Ausprägung die Anzahl der Beobachtungen gezählt:\nimport pandas as pd df = pd.read_csv(\u0026#34;../data/Library_Usage.csv\u0026#34;) df[\u0026#39;Age Range\u0026#39;].value_counts()    Mit der Funktion value_counts() können Sie sich absolute Häufigkeitstabellen ausgeben lassen. Mit dem zusätzlichen Argumentaufruf normalize=True werden relative Häufigkeiten berechnet:\ndf[\u0026#39;Age Range\u0026#39;].value_counts(normalize=True)    Der Modus ist dabei die Merkmalsausprägung, die die meisten Beobachtungen besitzen: df[\u0026#39;Age Range\u0026#39;].mode()    Visualisierung Häufigkeitstabellen lassen sich idealerweise als Balkendiagramme visualisieren:\nimport pandas as pd import seaborn as sns %matplotlib inline sns.set() df = pd.read_csv(\u0026#34;../data/Library_Usage.csv\u0026#34;) df[\u0026#39;Age Range\u0026#39;].value_counts(normalize=True).plot(kind=\u0026#34;bar\u0026#34;)    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/",
	"title": "Modul 3",
	"tags": [],
	"description": "",
	"content": "Daten analysieren und verstehen Herzlichen Willkommen zum dritten Modul Daten analysieren und darstellen des Data Librarian Zertifikationskurs. In diesem Modul möchten wir Ihnen einen praktischen Einblick in die Datenanalyse mit der Programmiersprache Python geben.\nNachdem Sie auf den Präsenztagen schon die grundlegenden Werkzeuge und Programmiertechniken kennen gelernt haben, werden Sie sich in diesem Modul anhand von praktischen Beispielen und Aufgaben Grundlagen der deskriptiven Statistik, der Datenvisualisierung und des Maschinellen Lernens aneignen. Dabei können Sie Ihre Programmier- und Datenanalysekenntnisse in Python verbessern und im bibliothekarischen Kontext anwenden.\nDie Kurseinheiten bauen aufeinander auf. Wir empfehlen Ihnen deswegen durch die Inhalte dieses Moduls mit den Pfeiltasten zu navigieren. In der linken Navigationsleiste wird Ihr Fortschritt gespeichert.\nDiese Webseite ist für alle Endgeräte optimiert. Sie können deswegen auch Ihr Smartphone oder Tablet zum Lesen nutzen.\nStarten Sie nun mit der ersten Kurseinheit indem Sie auf klicken oder mit den Pfeiltasten navigieren.\n  "
},
{
	"uri": "https://bonartm.github.io/data-librarian/organisation/packages/numpy/",
	"title": "numpy",
	"tags": [],
	"description": "Effizientes Handling und Bearbeitung von numerischen Arrays.",
	"content": "numpy bietet den array als zentrale Datenstruktur. Mit ihm lassen sich numerische Daten effizient im Arbeitsspeicher (RAM) erstellen, ein- und auslesen, bearbeiten und aggregieren.\nNumpy bietet neben dem array viele Funktionen an, mit denen sich effizient Berechnungen auf diesen durchführen lassen können. Außerdem wird die klassische Matrizenrechnung unterstützt.\n# import the library and give it a shorter name \u0026#39;np\u0026#39; import numpy as np # create 100 randomly distributed numbers X = np.normal.random(size=100) # transform X into a 2-dimensional array of size 20x5 X.reshape((20, 5)) # calculate the matrix dot product: X*X\u0026#39;, where X\u0026#39; is the transformation of X X.dot(X.T)    Beispielsweise kann ein Bild als dreidimensionales numpy array dargestellt werden: Die ersten zwei Dimensionen beschreiben die Farbintensität der Pixel auf einer zweidimensionalen Fläche. Die dritte Dimension speichert die jeweiligen Pixelwerte für die Farbkanäle rot, grün und blau.\n  https://www.oreilly.com/library/view/elegant-scipy/9781491922927/ch01.html\n  "
},
{
	"uri": "https://bonartm.github.io/data-librarian/basics/pandas/series/",
	"title": "Series und DataFrames",
	"tags": [],
	"description": "",
	"content": "Series und DataFrames sind die zentralen Datenstrukturen in Pandas. Series sind wie Standard Python-Listen, mit dem wichtigen Unterschied, dass Series nur Werte eines Datentyps enthalten können.\nimport pandas as pd x = pd.Series([34, 12, 23, 45]) print(x)    Ein Datentyp ist die grundlegende Einheit, in der einzelne Werte in einer Programmiersprache vom Computer gespeichert werden können. Beispiele für Datentypen in Python sind: float, int, str, bool oder datetime.\n  Ein DataFrame fasst mehrere Series gleicher Länge zu einer Datentabelle mit Zeilen (Beobachtungen), Spalten (Variablen) und Spaltennamen (Variablennamen) zusammen.\nDataFrames können beispielsweise aus Python-Dictionaries gebildet werden:\nimport pandas as pd data = {\u0026#39;month\u0026#39;: [\u0026#39;Jan\u0026#39;, \u0026#39;Feb\u0026#39;, \u0026#39;Mar\u0026#39;], \u0026#39;temp\u0026#39;: [\u0026#39;-5\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;]} df = pd.DataFrame(data) print(df)    In der praktischen Datenanalyse werden Sie nur selten DataFrames oder Series manuell erstellen, sondern im Computer abgespeicherte Datentabellen aus anderen Formaten, wie Excel oder .csv einlesen.\n"
},
{
	"uri": "https://bonartm.github.io/data-librarian/basics/ml/statistic/",
	"title": "Statistik",
	"tags": [],
	"description": "",
	"content": " Justus Perthes (1838): Rhein, Elbe und Oder   Statistik ist die traditionelle Wissenschaft von der Erhebung und Analyse von Daten. Sie verfügt über eine großes theoretisches und mathematisches Fundament und lässt sich in die Teilgebiete deskriptive (Beschreiben), explorative (Suchen) und schließende (Induktion) Statistik unterteilen.\nLesen Sie mehr über die Grundlagen der Statistik im Kapitel Grundbegriffe der Statistik.\n"
},
{
	"uri": "https://bonartm.github.io/data-librarian/basics/pandas/io/",
	"title": "Ein- und Ausgabe",
	"tags": [],
	"description": "",
	"content": "Die Ein- und Ausgabe von Daten in pandas ist umfangreich aber einfach. Um eine .csv Datei einzulesen und in einer Variable zu speichern verwenden Sie die Funktion read_csv:\nimport pandas as pd df = pd.read_csv(\u0026#34;../data/Library_Usage.csv\u0026#34;) df.head()    Um einen eingelesenen Datensatz beispielsweise im .json Textformat zu speichern verwenden Sie die Funktion to_json:\ndf.to_json(\u0026#34;../data/Library_Usage.json\u0026#34;)     Manche Funktion in pandas sind statische Funktionen, d.h. sie sind an kein konkretes Objekt gebunden. Beispiele: pd.read_csv, pd.to_numeric, pd.crosstab. Andere Funktionen sind an ein Objekt, welches mit einer Variable referenziert wird, gebunden. Dies kann ein konkreter DataFrame mit dem Variablennamen df oder eine Series sein. Beispiele: df.to_csv, df.corr, df.head. Machen Sie sich mit dem Unterschied vertraut. Was bedeuten pd und df in den Beispielen?    Datenrundreise  Informieren Sie sich hier über die verschiedenen Funktionen zur Ein- und Ausgabe. Lesen Sie den Datensatz \u0026quot;../data/Library_Usage_Small.csv\u0026quot; ein. Er enthält nur die ersten 100 Zeilen des originalen Datensatzes (aus Performancegründen). Speichern sie den DataFrame als .json ab. Lesen Sie die .json ein und speichern Sie den DataFrame als .html Tabelle ab. Die .html Datei können Sie danach auch mit einem Browser öffnen. Lesen Sie dann die .html Datei ein und speichern Sie den DataFrame als .xlsx Datei ab. Lesen Sie nun die .xlsx Datei ein und speichern Sie den DataFrame wieder als .csv ab. Achten Sie darauf, den ursprünglichen originalen Datensatz nicht zu überschreiben. Vergleichen Sie die originale .csv Version mit der Version, nach der Datenrundreise. Ist alles gleich geblieben?    Exkurs: Was bedeutet einlesen? Die Festplatte des Computers dient zur persistenten Speicherung von Dateien. Auch wenn der Strom weg ist, bleiben diese auf der Festplatte erhalten. Diese Speicherfähigkeit hat ihren Preis: Die Zugriffszeiten, d.h. die Zeit die die Festplatte benötigt um z.B. Zeilen einer Textdatei zu lesen und die Werte an den Prozessor zu übergeben, sind hoch.\nDeswegen gibt es neben dem Festplattenspeicher auch noch den Arbeitsspeicher (RAM). Dessen Zugriffszeiten sind wesentlich schneller, die Daten sind jedoch nicht persistent. Wenn Sie z.B. eine Tabelle mit Excel öffnen, dann werden die Daten von der Festplatte in den Arbeitsspeicher geladen. Das gleiche, nur ohne graphische Oberfläche, passiert, wenn Sie Daten mit dem pandas Paket einlesen.\nDa normalerweise der Datensatz komplett in den Arbeitsspeicher geladen werden muss, können prinzipiell nicht beliebig große Datenmengen bearbeitet werden.\n Finden Sie heraus, wie viel freier Arbeitsspeicher Ihr Computer hat (Das Betriebssystem und Hintergrundprogramme verbrauchen auch RAM). Wie viele int64 Werte, also Zahlen, die 8 Byte (=64 Bit) Speicher benötigen, können Sie damit theoretisch in den Arbeitsspeicher laden? Wie viele Beobachtungen kann eine Tabelle mit 100 Variablen damit maximal theoretisch haben, damit Sie diese noch bearbeiten können? Nutzen Sie die Funktion memory_usage um sich den tatsächlich benötigten Speicher eines DataFrames oder einer Series anzeigen zu lassen.    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/basics/ml/ml/",
	"title": "Machine Learning",
	"tags": [],
	"description": "",
	"content": "Seit der Erfindung des Personal Computers und des Internets werden statistische Probleme immer komplexer und größer und die neuen Datenmengen erfordern neue effiziente Strukturen zum Speichern und Auffinden der Informationen.\nMaschinelles Lernen (\u0026ldquo;Statistical Learning\u0026rdquo;) bedeutet in diesem Kontext relevante und signifikante Muster und Trends aus den Daten zu extrahieren und die Daten \u0026ldquo;zu verstehen\u0026rdquo;. Dabei spielen Computer und deren wachsende Rechenpower eine immer größere Rolle. Sie haben die klassische Statistik revolutioniert und es sind vor allem Ingenieure und Informatiker, die die Weiterentwicklung der Disziplin heutzutage vorantreiben.1\nIm Gegensatz zur klassischen Statistik, steht im Maschinellen Lernen auch viel stärker die Vorhersagekraft und Generalisierbarkeit von Methoden im Vordergrund und weniger ein Verständnis der kausalen Zusammenhänge.\nWährend die klassische Statistik oft an der Interpretation von kausalen Einflüssen einzelner Faktoren auf ein Ergebnis interessiert ist, sind Black-Box Modelle im Maschinellen Lernen viel präsenter: Ziel ist es oft, möglichst gute Prognosen, beispielsweise bei der Gesichtserkennung, zu machen. Dabei spielt letztendlich das genaue Modell und die Gewichte für die Variablen nur eine untergeordnete Rolle.\nTeachable Machine von Google ermöglicht das Trainieren von Machine Learning Modellen im eigenen Web-Browser ohne das Programmiercode geschrieben werden muss. Somit wird ML auch ohne Vorkenntnisse erfahrbar und man bekommt ein gutes Gespür für die Möglichkeiten und Grenzen dieser Methoden.\nExperimentieren Sie zum Beispiel mit dem Bild-Klassifikator.\n  Maschinelles Lernen und Statistik haben viele Schnittmengen. ML kann auch als Teilgebiet der Künstlichen Intelligenz aufgefasst werden. KI wird definiert als: Die Bemühung, intellektuelle Aufgaben, die normalerweise von Menschen durchgeführt werden, zu automatisieren.2\nKI kann auch allein mit durch Programmierer fest eingebauten Regeln entstehen und ist damit bei klaren logischen Problemen auch sehr erfolgreich (z.B. Schachcomputer). Systeme des Maschinellen Lernens werden hingegen trainiert: Es werden viele Beispiele mit Eingabe- und Ausgabedaten vorgegeben und das System lernt selbstständig die \u0026ldquo;Regeln\u0026rdquo;. Diese können dann auf neue Daten, von denen keine Ausgabe bekannt ist, angewandt werden, um Prognosen zu erstellen.2\n  Hastie, Tibshirani, Friedman (2017): The Elements of Statistical Learning, Springer.\n Francois Chollet (2018): Deep Learning with Python, Manning.\n   "
},
{
	"uri": "https://bonartm.github.io/data-librarian/basics/pandas/columns/",
	"title": "Auswahl und Erstellung von Spalten",
	"tags": [],
	"description": "",
	"content": "Die Spalten eines DataFrames werden über einen Spaltenindex referenziert. Üblicherweise besteht der Spaltenindex aus Spaltennamen in Textform:\nimport pandas as pd df = pd.read_csv(\u0026#34;../data/Library_Usage.csv\u0026#34;) df.columns    Einzelne Spalten können wie bei einem Python Dictionary mit df[\u0026lt;name\u0026gt;] ausgewählt werden. Mehre Spalten mit df[[\u0026lt;name1\u0026gt;, \u0026lt;name2]]:\ndf[\u0026#39;Total Renewals\u0026#39;] df[[\u0026#39;Total Renewals\u0026#39;, \u0026#39;Total Checkouts\u0026#39;]] filter = [\u0026#39;Total Renewals\u0026#39;, \u0026#39;Total Checkouts\u0026#39;] # auxiliary variable df[filter]    Neue Spalten können mit einer Zuweisung (=) erstellt werden: df[\u0026#39;dummy_variable\u0026#39;] = 5    Berechnungen auf schon bestehenden Variablen können auch direkt einer neuen Spalte zugeordnet werden:\nimport numpy as np df[\u0026#39;is_adult\u0026#39;] = df[\u0026#39;Patron Type Definition\u0026#39;] == \u0026#39;ADULT\u0026#39; df[\u0026#39;log_renewals\u0026#39;] = np.log(df[\u0026#39;Total Renewals\u0026#39;])    Im ersten Beispiel wurde zuerst die Anweisung df['Patron Type Definition'] == 'ADULT' durchgeführt. Das implizite Ergebnis dieser Anweisung ist eine Series mit booleschen Werten (True oder False). Die neu erstellte Series wird dann in einer neuen Spalte is_adult dem DataFrame angehängt.\nIm zweiten Beispiel wurde der Logarithmus auf den Werten der Spalte Total Renewals berechnet und einer neuen Spalte log_renewals zugewiesen.\n Die Spalte Circulation Active Year ist als Text und nicht als Zahl abgespeichert! Konvertieren Sie die Spalte in ein numerisches Format: pd.to_numeric(df[\u0026#39;Circulation Active Year\u0026#39;], errors=\u0026#39;coerce\u0026#39;) Überschreiben Sie die ursprüngliche Variable mit den neuen Werten.\n Erstellen Sie eine neue Variable 'Membership Duration' die die Zeit in Jahren zwischen der Registrierung im System und der letzten Ausleihaktivität des Kunden enthält.    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/basics/ml/data-science/",
	"title": "Data Science",
	"tags": [],
	"description": "",
	"content": "Grundsätzlich ist ein Data Scientist jemand, die oder der Wissen und Erkenntnisse aus strukturierten und unstrukturierten Daten gewinnt. Data Science ist eine interdisziplinäre Disziplin, denn sie liefert eine Sammlung an quantitativen Methoden- und Algorithmen, die in einem Fachgebiet angewandt werden können 2. Damit liegt Data Science irgendwo in der Schnittmenge von Mathe/ Statistik, Programmierung (\u0026ldquo;Hacking skills\u0026rdquo;) und Fachwissen (\u0026ldquo;domain knowledge\u0026rdquo;/ \u0026ldquo;substantive expertise\u0026rdquo;).\nAufgrund der stark angewachsenen Mengen an unstrukturierten Daten aus heterogenen Datenquellen (Text, Bilder, Sensoren, Netzwerke, Videos, \u0026hellip;) reichen die Methoden und Fertigkeiten, die die Statistik traditionellerweise liefert und vermittelt, nicht mehr aus, um diese Daten effizient zu strukturieren, aggregieren, kombinieren, analysieren und visualisieren zu können:\n A Data Scientist is someone who knows more statistics than a computer scientist and more computer science than a statistician 1.\n taken from http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram\n  Joel Grus (2019): Data Science from Scratch, O'Reilly.\n Jake VanderPlas (2019): Python Data Science Handbook, O'Reilly\n   "
},
{
	"uri": "https://bonartm.github.io/data-librarian/basics/pandas/rows/",
	"title": "Auswahl von Zeilen",
	"tags": [],
	"description": "",
	"content": "Die Zeilen eines DataFrames werden über einen Zeilenindex (loc[]), über die aufsteigenden Zeilennummern (iloc[]) oder über logische Ausdrücke ([] oder loc[]) referenziert.\nHier wird zuerst der wichtigste letzte Fall näher betrachtet:\nimport pandas as pd df = pd.read_csv(\u0026#34;../data/Library_Usage.csv\u0026#34;) df[df[\u0026#39;Total Checkouts\u0026#39;] \u0026gt; 10000]    Der Ausdruck df['Total Checkouts'] \u0026gt; 10000 wird zuerst ausgewertet und ergibt eine boolesche Series mit Einträgen True wenn die Beobachtung mehr als 1000 Ausleihen getätigt hat und False sonst.\nMit einer booleschen Series lassen sich dann die Zeilen des DataFrame auswählen: Es werden genau die Zeilen zurückgegeben, bei denen die Series` True` Werte enthält.\nAnstatt alles in einer Zeile zu schreiben, können wir auch eine Hilfsvariable erstellen, die den booleschen Vektor zwischenspeichert:\nfilter = df[\u0026#39;Total Checkouts\u0026#39;] \u0026gt; 10000 df[filter]    Für den booleschen Zeilenfilter können komplexe logische Ausdrücke unter Zuhilfenahme der Operatoren \u0026lt;, \u0026gt;, \u0026amp;, |, == u.s.w. gebildet werden:\nfilter = (df[\u0026#39;Patron Type Definition\u0026#39;] == \u0026#39;SENIOR\u0026#39;) \u0026amp; (df[\u0026#39;Notice Preference Definition\u0026#39;] == \u0026#39;email\u0026#39;) df[filter]    Logische Operatoren    Ausdruck Beschreibung     \u0026lt;/ \u0026lt;= kleiner/ kleiner gleich   \u0026gt; / \u0026gt;= größer/ größer gleich   == gleich   != ungleich   \u0026amp; elementweises logisches und   | elementweises logisches oder   ~ elementweise logische negation       Filter Sie den Datensatz nach Kindern unter 10 Jahren. Wie viele Einträge erhalten Sie? Gibt es Personen mit mehr als 20000 Ausleihen? Wie viele Personen stammen aus dem Norden San Franciscos (Supervisor Districts 1, 2 und 3)? Nutzen Sie die Funktion Series.isin().    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/basics/pandas/na/",
	"title": "Fehlende Werte",
	"tags": [],
	"description": "",
	"content": "Real erhobene Daten sind meistens unsauber und fehlerhaft. Ein häufiges Problem dabei sind fehlende Werte, also Beobachtungen für die manche Merkmale nicht erhoben wurden. In jedem Datensatz werden fehlende Werte anders gekennzeichnet, aber man findet meistens diese Kodierungen wieder: \u0026quot;-999\u0026quot;, \u0026quot;NA\u0026quot;, \u0026quot; \u0026quot;, \u0026quot;None\u0026quot;, \u0026quot;NULL\u0026quot;, \u0026quot;#N/A\u0026quot;.\nWenn beispielsweise der Mittelwert einer statistischen Variable berechnet wird, so muss entschieden werden, wie mit fehlenden Werten umgegangen werden soll: Sollen die Werte entfernt werden? Sollen die fehlenden Werte durch durch einen bestimmten Wert ersetzt werden?\nIn DataFrames werden fehlende Werte durch zwei Arten angezeigt: Das Schlüsselwort NaN (\u0026ldquo;Not a Number\u0026rdquo;) wird in numerischen Series verwendet. Das Schlüsselwort None in nicht-numerischen.\nBeim Einlesen von Daten (siehe z.B. die read_csv Funktion) können mit dem Argument na_values zusätzliche Kodierungen für fehlerhafte Werte mit angegeben werden.\nFallbeispiel Der Library Usage Datensatz enthält die Kodierung \u0026quot;None\u0026quot; für fehlende Werte. Diese werden von pandas beim Einlesen von numerischen Spalten nicht richtig erkannt:\nimport pandas as pd df = pd.read_csv(\u0026#34;../data/Library_Usage.csv\u0026#34;) df[\u0026#39;Circulation Active Year\u0026#39;] Obwohl die Spalte 'Circulation Active Year' eigentlich numerisch ist, wird Sie von pandas als Text erkannt. Möchten Sie z.B. 2019 - df['Circulation Active Year'] berechnen, so werden Sie eine Fehlermeldung erhalten, da für Text-Werte keine Substraktionen durchgeführt werden können.\nUm das Problem zu beheben können Sie auf zwei Arten vorgehen. Sie können schon beim Einlesen, die Kodierung für fehlende Werte mit angeben:\ndf = pd.read_csv(\u0026#34;../data/Library_Usage.csv\u0026#34;, na_values=\u0026#34;None\u0026#34;) df[\u0026#39;Circulation Active Year\u0026#39;] Oder Sie führen nach dem Einlesen eine explizite Umwandlung des Datentyps durch:\ndf = pd.read_csv(\u0026#34;../data/Library_Usage.csv\u0026#34;, na_values=\u0026#34;None\u0026#34;) df[\u0026#39;Circulation Active Year\u0026#39;] = pd.to_numeric(df[\u0026#39;Circulation Active Year\u0026#39;], errors=\u0026#39;coerce\u0026#39;) df[\u0026#39;Circulation Active Year\u0026#39;]  Was unterscheidet den Wert None vom Wert \u0026quot;None\u0026quot;? Was den Wert 5 vom Wert \u0026quot;5\u0026quot;? Was den Wert \u0026quot;NaN\u0026quot; vom Wert NaN? Ist True und \u0026quot;True\u0026quot; das gleiche?\n  Behandlung von Fehlenden Werten Pandas bietet die nützlichen Funktionen isna(), notna(), dropna() und fillna() an um fehlende Werte zu identifizieren, zu entfernen oder mit anderen Werten zu ersetzen.\nFilter df[df[\u0026#39;Age Range\u0026#39;].isna()] df[df[\u0026#39;Age Range\u0026#39;].notna()]    Entfernen # drops all rows that contain missing values df.dropna() # drops all missing values in this series df[\u0026#39;Age Range\u0026#39;].dropna()     Lesen Sie den Datensatz ein und erstellen Sie einen DataFrame der keine Beobachtungen mit fehlenden Werten mehr enthält. Speicher Sie diesen unter dem Namen Library_Usage_Clean.csv ab. Wie viele Beobachtungen wurden dabei entfernt?    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/organisation/",
	"title": "Kursorganisation und Vorbereitungen",
	"tags": [],
	"description": "",
	"content": "21.01 – 26.01 Kursorganisation und Vorbereitungen Diese Einheit gibt einen Überblick über die Kursinhalte, wichtige Termine und die benötigte Software und Python-Pakete. Viele der hier besprochenen Dinge werden Ihnen schon bekannt vorkommen und die Software haben Sie voraussichtlich auch schon auf Ihrem Rechner installiert.\nZiele  Installieren Sie Anaconda mit Python 3.7 auf Ihrem Rechner Erstellen Sie einen Projektordner und fügen Sie einen Datensatz Ihrem Projektordner hinzu Stellen Sie sicher, dass Python Notebooks lokal ausgeführt werden können  "
},
{
	"uri": "https://bonartm.github.io/data-librarian/organisation/modules/",
	"title": "Kurseinheiten",
	"tags": [],
	"description": "",
	"content": "Wir haben das Modul in wöchentliche Einheiten, die jeweils ein Gebiet aufgreifen und vertiefen, unterteilt. Sie können sich die Zeit für die Bearbeitung der Einheiten selber aufteilen, sollten aber jede Einheit am Ende der jeweiligen Woche abgeschlossen haben. Am Ende einer Woche wird die nächste Einheit auf dieser Webeite freigeschaltet.\nJede Einheit umfasst ein kleines praktisches Projekt, welches Sie in Form eines Jupyter Notebooks bearbeiten und aufbereiten. Ihr Notebook können Sie einreichen, um Feedback von den Kursleitern zu erhalten.\nDer erste Teil des Moduls (21-01 - 16.02) wird von Malte Bonart betreut und behandelt grundlegende klassische Konzepte der angewandten Statistik. Der zweite Teil des Moduls (10.02 - 04.03) wird von Konrad Förstner betreut und gibt einen Überblick über Themen des Maschinellen Lernens.\nAm Präsenztag, der am 05.03.2020 stattfindet, werden wir im voraus gesammelte Fragen gemeinsam beantworten und diskutieren. Sie werden Zeit haben, an einem persönlichen Datenanalyseprojekt zu arbeiten. Die Kursleiter werden Sie dabei unterstützen und individuell betreuen. Am Ende des Präsenztages stellen alle KursteilnehmerInnen ihre Ergebnisse in einer Kurzpräsentation vor.\nDen Source Code für diese Webseite und die weiteren Kursmaterialien finden Sie in einem öffentlichen GitHub Repository. Sie können sich optional das komplette Repository herunterladen indem sie folgenden Befehl in der Kommandozeile ausführen:\ngit clone https://github.com/bonartm/data-librarian.git    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/organisation/schedule/",
	"title": "Termine",
	"tags": [],
	"description": "",
	"content": "Hier finden Sie einen Überblick über die einzelnen Moduleinheiten. Die Tabelle können Sie sich auch als .pdf anzeigen lassen und ausdrucken:\n  Überblick Modul 3   überblick_modul3.pdf  (25 ko)       Datum Titel Ziele     21.01 – 26.01 Vorbereitung Installieren Sie die benötigte Software Laden Sie die Kursmaterialien und Datensätze herunter Stellen Sie sicher, dass Python Notebooks lokal ausgeführt werden können   27.01 – 02.02 Grundlagen und deskriptive Statistik I Beschreiben Sie Datensätze mit dem statistischen Grundvokabular Lesen Sie Datensätze als DataFrames in Python ein Filtern Sie DataFrames nach Spalten oder Zeilen Erstellen Sie absolute und relative Häufigkeitstabellen Berechnen Sie grundlegende Lagemaße   03.02 – 09.02 Deskriptive Statistik II und Visualisierung Berechnen Sie grundlegende Streuungsmaße Berechnen Sie Statistiken für bivariate Verteilungen Erstellen Sie einfache Visualisierungen   10.02 – 16.02 Inferenzstatistik / Maschinelles Lernen I Berechnen und visualisieren Sie Konfidenzintervalle für den Mittelwert Beschreiben Sie die Unterschiede zwischen Supervised und Unsupervised Learning   17.02 – 23.02 Maschinelles Lernen II Beschreiben Sie grundlegende Funktionsweisen und Konzepte von scikit-learn Führen Sie eine Regression, Klassifikation oder Clustering mit scikit-learn durch   24.02 – 01.03 Maschinelles Lernen III Beschreiben Sie die Funktionsweise von Text-Analyse mit NLTK oder spaCy Formulieren Sie einfache quantitative Fragen für den Projekttag als Expose (max. 1 Seite Text)   02.03 – 04.03 Vorbereitung Präsenztag Suchen Sie nach geeigneten Daten für den Projekttag Schicken Sie Ihre inhaltlichen und fachlichen Fragen an die Kursleiter   5.03 Präsenztag Nehmen Sie an der Frage und Antwortrunde teil Finden Sie geeignete Daten zum Lösen der Fragen Beantworten Sie Ihre Frage mit den gelernten statistischen Tools Bereiten Sie die Ergebnisse in Form einer Visualisierung auf Stellen Sie die Ergebnisse in einer Kurzpräsentation in Ihrer Gruppe vor (\u0026lt; 5 Minuten)    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/organisation/anaconda/",
	"title": "Conda und Anaconda",
	"tags": [],
	"description": "",
	"content": "Conda ist eine freie und offene Softwarepaketverwaltung für Python. Neben der Möglichkeit, Pakete (packages, libraries) für Python aus dem Internet zu installieren, können mit conda virtuelle Umgebungen (environments) angelegt werden. Diese Umgebungen beinhalten nur die Pakete und Python Versionen, die für ein spezifisches Projekt gebraucht werden. Umgebungen können mit anderen Personen geteilt werden, sodass sichergestellt ist, dass alle Programmierer mit den gleichen Paketen und Versionen arbeiten, auch wenn sie unterschiedliche Systeme (Windows, Linux, MacOS) verwenden.\nAnaconda basiert auf conda. Mit Anaconda werden eine Vielzahl von Paketen, die für die Datenanalyse gebraucht werden, schon vorinstalliert. Außerdem bietet Anaconda eine vorinstallierte Entwicklungsumgebung (Spyder IDE) und eine vorinstallierte Version von Jupyter, mit der Notebooks gestartet werden können.\n Informieren Sie sich über die Unterschiede von Anaconda und Miniconda! Wenn noch nicht geschehen, können Sie Anaconda hier für Ihr Betriebssystem herunterladen. Wir verwenden die Version für Python 3.7. Öffnen Sie den mit Anaconda installierten Anaconda Navigator und verschaffen Sie sich einen Überblick über die vorhandenen Programme.    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/organisation/notebooks/",
	"title": "Jupyter Notebooks",
	"tags": [],
	"description": "",
	"content": "Die Projekt-Aufgaben und Code-Beispiele in diesem Modul werden über Jupyter Notebooks erstellt und verteilt.\nJupyter Notebooks bieten eine browserbasierte graphische Schnittstelle zur Python Programmierumgebung. Deswegen können Notebooks auf jedem System gestartet werden, man benötigt dazu nur einen Web-Browser und eine lokale installierte Version von Python.\nDarüber hinaus bieten Notebooks die Möglichkeit Text, Visualisierungen und Code in einer integrierten Datei zu erstellen. Somit können einfach statistische Reports und Analysen erstellt werden. Die Replizierbarkeit der Ergebnisse ist auch gewährleistet, da jede Person, die Programmierschritte im Notebook auf ihrem Rechner wiederholen kann.\nNotebooks bestehen immer aus Text oder Code Zellen (cells). Der Python Code in den Zellen kann ausgeführt werden und das Ergebnis wird direkt im Notebook angezeigt. Somit eignen sich Notebooks, um mit Code interaktiv zu experimentieren und für andere Personen aufzubereiten.\nJupyter Notebook enthält einen Dateimanager mit dem Sie durch die Ordner und Dateien Ihres Systems navigieren können. Mit einem Klick auf eine Notebook-Datei öffnet sich ein neues Browser-Tab mit dem Notebook.\n Laden Sie dieses Notebook herunter (Rechtsklick -\u0026gt; Ziel/Link speichern unter\u0026hellip;) Starten Sie Jupyter Notebook über die Kommandozeile oder über den Anaconda Navigator Navigieren Sie zu dem Speicherort des Notebooks und öffnen Sie es. Markieren Sie die Code-Zelle und führen Sie sie mit einem Klick auf den Run Button oder mit der Tastenkombination Strg`+Enter` aus Versuchen Sie, die Farbe der Punkte im Plot von Grün auf Rot zu ändern Fügen Sie das Datum und Ihren Namen der Text-Zelle hinzu     Notebook-Dateien erkennen Sie immer an der Dateiendung .ipynb. Diese Dateien können Sie in Jupyter mit dem integrierten Dateimanager öffnen. Jupyter starten Sie entweder über den Anaconda Navigator oder indem Sie den folgenden Befehl in Ihrer Kommandozeile ausführen (Die Kommandozeile danach nicht wieder schließen!):  jupyter notebook  Rufen Sie http://localhost:8890 in Ihrem Browser auf, um zur Oberfläche von Jupyter zu gelangen.      Jupyter Notebook   tutorial_jupyter.ipynb  (21 ko)    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/organisation/dataset/",
	"title": "San Francisco Library Usage",
	"tags": [],
	"description": "",
	"content": "Im ersten Teil des Moduls werden Sie einen offenen Kundendatensatz der Bibliothek in San Francisco analysieren.\n The Integrated Library System (ILS) is composed of bibliographic records including inventoried items, and patron records including circulation data. The data is used in the daily operation of the library, including circulation, online public catalog, cataloging, acquisitions, collection development, processing, and serials control. This dataset represents the usage of inventoried items by patrons \u0026hellip; (Abstract taken from here)\n  Besuchen Sie das offene Daten-Portal der Stadt San Francisco und informieren Sie sich über den Datensatz Erstellen Sie einen Ordner auf Ihrem Computer. Dieser Ordner wird Ihr Projektordner für dieses Modul. Dort legen Sie alle Datensätze und Jupyter Notebooks ab. Erstellen Sie einen Unterordner ./data/ und einen Unterordner ./notebooks/ innerhalb Ihres Projektordners. Laden Sie den Datensatz Library_Usage.csv aus dem Internet herunter und speichern Sie ihn im Projektordner im Unter-Ordner ./data/ ab. Stellen Sie sicher, dass Ihr Projektordner die folgende Verzeichnisstruktur aufweist:  data-librarian-3 ├── data │ └── Library_Usage.csv ├── notebooks │ └── tutorial_jupyter.ipynb    In dieser Excel Tabelle finden Sie eine detallierte Erklärung der einzelnen Variablen des Datensatzes.\n    books by 1 brian is licesed under CC BY-NC-SA 2.0\n  "
},
{
	"uri": "https://bonartm.github.io/data-librarian/organisation/packages/",
	"title": "Python Pakete und Bibliothekten",
	"tags": [],
	"description": "",
	"content": "Die folgende Liste gibt einen kurzen Überblick über die wichtigsten Python Bibliotheken, von denen Sie manche im Modul näher kennenlernen werden.\nIm ersten Teil des Modules werden wir mit pandas und seaborn arbeiten.\n numpy  Effizientes Handling und Bearbeitung von numerischen Arrays.\n pandas  Bearbeitung, Transformation, Aggregation und Zusammenfassung von Datensätzen. Baut auf numpy auf.\n matplotlib  Bietet 2D Plotting Funktionalitäten.\n seaborn  Verbesserung und Weiterentwicklung der matplotlib Bibliothek.\n scipy  Funktionen und Methoden aus der Statistik.\n scitkit-learn  Bietet Funktionen und Methoden für maschinelles Lernen.\n "
},
{
	"uri": "https://bonartm.github.io/data-librarian/organisation/quiz_intro/",
	"title": "Recap: Quiz",
	"tags": [],
	"description": "",
	"content": " .quiz fieldset { border-color: black; border-width: 10px; margin-bottom: 1em; } .quiz legend { font-size: 105%; font-weight: 600; padding-left: 15px; padding-right: 15px; padding-top: 15px; } .quiz label { display: block; line-height: 1.75em; } .quiz input[type=\"radio\"] { margin-right: 10px; page-break-after: avoid; page-break-before: avoid; } .quiz input[type=\"submit\"] { background: black; color: white; display: block; font-size: 120%; font-weight: 600; height: 2.5em; margin-top: 2em; text-transform: uppercase; width: 100%; } .quiz table { color: white; font-weight: bold; margin: 1em auto 2em auto; width: 100%; } .quiz td { padding: 5px 15px; text-align: left; width: 60px; } .quiz td.missing-label, .quiz td.missing-score { background: #CECBC2; } .quiz td.right-label, .quiz td.right-score { background: #74b559; } .quiz td.wrong-label, .quiz td.wrong-score { background: #D01F3C; }    var quiz = new Quiz(\"vorbereitung_quiz\", questions, {\"shuffle\": true});   "
},
{
	"uri": "https://bonartm.github.io/data-librarian/basics/ml/",
	"title": "Statistik, Data Science und Machine Learning",
	"tags": [],
	"description": "",
	"content": "Seit einigen Jahren sind Data Science und Machine Learning zu alltäglichen Begriffen geworden. Studiengänge im Bereich Data Science werden neu eingerichtet oder schon bestehende Abschlüsse umbenannt. Maschinelles Lernen und insbesondere Themen wie künstliche Intelligenz, Neuronale Netze und Deep Learning sind Thema in Zeitungen und Nachrichten. Und auch der Zertifikationskurs Data Librarian spiegelt diese Entwicklung wieder.\n Was bedeuten diese Begriffe und wo ist dabei die Statistik einzuordnen?\n Im folgenden wird ein kurzer Überblick über die Begriffe gegeben. Für Interessierte gibt es Verweise zu weiteren Quellen.\n"
},
{
	"uri": "https://bonartm.github.io/data-librarian/basics/basic_terms/",
	"title": "Grundbegriffe der Statistik",
	"tags": [],
	"description": "",
	"content": "In der Kursumgebung finden Sie das Einführungskapitel des Buchs Statistik: Der Weg zur Datenanalyse zum alleinigen persönlichen Gebrauch hier im Kurs hinterlegt.1\nDer Text gibt einen Einstieg in die Aufgaben und Anwendungsbereiche der Statistik und erklärt die grundlegenden Begriffe, mit denen Daten und Datensätze charakterisiert werden können.\nBeantworten und diskutieren Sie folgende Fragen konkret für den San Francisco Library Usage Datensatz. Halten Sie Ihre Ergebnisse in Stichpunkten in einem Jupyter Notebook fest.\n Wie viele Merkmale besitzt der Datensatz? Wie groß ist die Stichprobengröße des Datensatzes? Wer oder was sind die Merkmalsträger? Von wann bis wann wurden die Daten erhoben? Wie lässt sich die Grundgesamtheit beschreiben? Handelt es sich um eine Vollerhebung? Welche Merkmale sind stetig? Welche diskret? Welchem Skalenniveau entsprechen die einzelnen Merkmale (Nominal-, Ordinal- oder Kardinalskala/ metrisch)? Enthält der Datensatz fehlende Werte? Handelt es sich um Querschnitts-, Längsschnitss- oder Paneldaten?      Fahrmeir, Ludwig, Christian Heumann, Rita Künstler, Iris Pigeot, and Gerhard Tutz. Statistik: Der Weg zur Datenanalyse. Springer-Verlag, 2016, https://www.springer.com/de/book/9783662503713.\n   "
},
{
	"uri": "https://bonartm.github.io/data-librarian/descriptive_statistics/univariate/",
	"title": "Univariate Verteilungen",
	"tags": [],
	"description": "",
	"content": "In der Statistik geben Verteilungen an, wie wahrscheinlich oder häufig eine bestimmte Merkmausausprägung oder Kombination von Merkmausausprägungen ist. Univariate Verteilung beschreiben dabei die Wahrscheinlichkeiten einer einzelnen statischen Variablen, während bivariate oder multivariate Verteilungen sich auf zwei oder mehr Variablen beziehen.\nEmpirische Verteilungen beziehen sich dabei auf die Häufigkeiten in konkreten Daten während theoretische Verteilungen als mathematischen Funktionen, die von einigen wenigen Parametern abhängen, vorliegen. Eine Hauptaufgabe der schließenden Statistik ist es, die beobachtbaren Daten so zu nutzen, dass die Parameter von theoretischen Verteilungen korrekt geschätzt werden können.\nStatistiken, wie der Mittelwert oder der Modus dienen zur Beschreibungen und Charakterisierung von Verteilungen in einigen wenigen Kennzahlen. Dabei gibt es häufig Statistiken, die nur für bestimmte Skalenniveaus Sinn machen.\nKategoriale (nominale und ordinale) Variablen werden in Häufigkeitstabellen zusammengefasst. Wichtige charakteristische Eigenschaften für metrische Variablen sind die zentrale Lage, die Streuung und die Symmetrie.\nIm Folgenden werden mit $x = x_1, \\dots, x_n$ eine univariate Reihe von Beobachtungen beschrieben, mit $n$ die Anzahl der Beobachtungen. $x_i$ beschreibt die Beobachtung an der i-ten Stelle.\n  "
},
{
	"uri": "https://bonartm.github.io/data-librarian/descriptive_statistics/univariate/mean/",
	"title": "Lagemaße",
	"tags": [],
	"description": "",
	"content": "Lagemaße beschreiben die Zentralität einer Verteilung. Das bekannteste Lagemaß ist der empirische Mittelwert:\n$$ \\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_i = \\frac{x_1 + x_2 + \\dots + x_n}{n} $$\nimport pandas as pd df = pd.read_csv(\u0026#34;../data/Library_Usage.csv\u0026#34;) df[\u0026#39;Total Checkouts\u0026#39;].mean()    Eine zweite wichtige Statistik ist der Median. Er ergibt sich aus dem Wert der Beobachtung, die die nach der Größe geordnete Messreihe in genau zwei gleich große Teile teilt. Für eine gerade Anzahl an Beobachtung wird der Mittelwert der Beobachtung an der Stelle $n/2$ und $n/2+1$ genommen:\n$$ x_{0.5} = \\begin{cases} x_{(n+1)/2}, \\text{n ungerade} \\\\\n\\frac{x_{n/2} + x_{n/2+1}}{2}, \\text{n gerade} \\end{cases} $$ für $x_1 \u0026lt; x_2 \u0026lt; \\dots \u0026lt; x_n$.\nBeispiel: Für $x=[8, 10, 11, 30]$ ist der Median $\\frac{x_2 + x_3}{2} = 10.5$.\ndf[\u0026#39;Total Checkouts\u0026#39;].median()     Schauen Sie sich den Mittelwert und den Median der Variable Total Checkouts an. Warum sind die beiden Werte so unterschiedlich? Was ziehen Sie daraus für Schlüsse für weitere statistische Analysen und Reports?    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/descriptive_statistics/univariate/variance/",
	"title": "Streuungsmaße",
	"tags": [],
	"description": "",
	"content": "Streuungsmaße geben an, wie stark die Daten einer Messreihe schwanken.\nDie Abweichung einer Beobachtung von dem Mittelwert der zugrundeliegenden Variable wird Abweichung genannt. Der Mittelwert über die quadrierten Abweichungen nennt man Varianz:\n$$ s^2_x = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i-\\bar{x})^2 $$\ndf[\u0026#39;Total Checkouts\u0026#39;].var()    Das Quadrieren der Abweichungen hat zur Folge, dass das Vorzeichen verschwindet und das große Abweichungen mehr Gewicht erhalten.\nIn der Formel wird durch $n-1$ anstatt durch $n$ geteilt. Dies ist theoretisch von Bedeutung, es hat aber in der Praxis meist keine Auswirkungen, wenn durch $n$ geteilt wird.\nDie Standardabweichungen ist die Wurzel der Varianz:\n$$ s_x = \\sqrt{s_x^2} $$\ndf[\u0026#39;Total Checkouts\u0026#39;].std()    Die Spannweite ist die Differenz zwischen dem maximalen und minmalem Wert\ndf[\u0026#39;Total Checkouts\u0026#39;].max() - df[\u0026#39;Total Checkouts\u0026#39;].min()    Quantile Sie haben schon den Median $x_0.5$ als Lageparameter kennengelernt. Er teilt die geordnete Verteilung in zwei genau gleich große Teile. Allgemeiner lassen sich analog dazu die Quantile definieren: $x_{0.75}$ teil die geordnete Verteilung im Verhältnis 3:1. Das heißt, dass 75% der Beobachtungen kleiner als $x_{0.75}$ und 25% größer sind. Das $x_{0.25}$ Quantil teilt die Reihe im Verhältnis 1:3. Hier sind 25% der Beobachtungen kleiner und 75% größer.\ndf[\u0026#39;Total Checkouts\u0026#39;].quantile(q=[0.25, 0.5, 0.75])    Daraus kann der Interquartilsabstand als Streuungsmaß abgeleitet werden: $$ x_{IQR} = x_{0.75} - x_{0.25} $$\ndf[\u0026#39;Total Checkouts\u0026#39;].quantile(q=0.75) - df[\u0026#39;Total Checkouts\u0026#39;].quantile(q=0.25)    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/descriptive_statistics/univariate/symmetrie/",
	"title": "Symmetrie",
	"tags": [],
	"description": "",
	"content": "  Verschiedene univariate Verteilungen   Neben der zentralen Lage- und Streuung einer Verteilung ist auch deren Symmetrie von Bedeutung.\nSymmetrie Eine symmetrische Verteilung zeichnet sich dadurch aus das Modus, Median und Mittelwert gleich sind.\nEine linksschiefe Verteilung liegt vor, wenn überproportional viele große Werte vorliegen.\nEine rechtsschiefe Verteilung ist durch überproprtional vielen kleinen Werten geprägt.\nZudem kann eine Verteilung auch Bi- oder Multimodal sein, das heißt es gibt mehre \u0026ldquo;Gipfel\u0026rdquo;.\nMit dem folgenden Beispiel können Sie ein Histogram der Anzahl der Ausleihen im Datensatz erstellen:\nimport pandas as pd import seaborn as sns sns.set() df = pd.read_csv(\u0026quot;../data/Library_Usage.csv\u0026quot;) df['Total Checkouts'].plot.hist(bins=100)     Charakterisieren Sie die Verteilung der Ausleihen pro Kunde Filter Sie den Datensatz nach Personen, die weniger als 100 Ausleihen haben und schauen Sie sich die Verteilung im Histogram an.      Related files   .ipynb_checkpoints  (4 ko)   distributions.ipynb  (231 ko)    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/descriptive_statistics/bivariate/",
	"title": "Bivariate Verteilungen",
	"tags": [],
	"description": "",
	"content": "Bisher haben Sie immer nur einzelne Variable betrachtet, zusammengefasst oder visualisiert. In vielen Fällen ist jedoch der Zusammenhang zwischen zwei Variablen von Interesse:\n Leihen ältere Bibliothekskunden im Schnitt mehr Bücher aus als jüngere? Führen Kunden, die häufiger Ausleihen tätigen, im Schnitt auch häufiger Verlängerungen durch? Nimmt die Anzahl der Ausleihen mit zunehmender Dauer der Mitgliedschaft ab?  Wenn zwei ordinale oder metrische Variablen voneinander abhängigen kann man zwischen einem positiven oder einem negativem Zusammenhang unterscheiden. Zwei Variablen, die keinen Zusammenhang aufweisen, nennt man auch statistisch unabhängige Variablen.\n"
},
{
	"uri": "https://bonartm.github.io/data-librarian/descriptive_statistics/bivariate/cross_tables/",
	"title": "Kreuztabellen",
	"tags": [],
	"description": "",
	"content": "Um zwei ordinale/ nominale Variablen miteinander zu vergleichen, eignen sich Kreuztabellen. Jeder Wert in der Kreuztabelle entspricht der Anzahl der Beobachtungen im Datensatz mit genau dieser Kombination.\nEine Kreuztabelle mit relativen Häufigkeiten erhält man, indem die absoluten Werte normalisiert.\nDie Normalisierung kann entweder dadurch erfolgen, dass durch die Anzahl der Beobachtungen $n$, durch die Anzahl der Beobachtungen in der Zeilenvariable oder durch die Anzahl der Beobachtungen in der Spaltenvariable geteilt wird.\n"
},
{
	"uri": "https://bonartm.github.io/data-librarian/descriptive_statistics/bivariate/correlation/",
	"title": "Korrelation",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://bonartm.github.io/data-librarian/basics/pandas/",
	"title": "Praktische Einführung in Pandas",
	"tags": [],
	"description": "",
	"content": "Grundlage der statistischen Analyse sind Datentabellen: Jede Zeile der Tabelle entspricht einer Beobachtung. Jede Spalte entspricht einer statistischen Variable. Neue Beobachtungen und Variablen können dadurch einfach an die schon bestehende Tabelle angefügt werden.\nAchten Sie darauf, dass wenn es um Statistik und Programmierung geht mit \u0026ldquo;Variable\u0026rdquo; zwei Dinge gemeint sind:\n Variable im Kontext eines statistischen Merkmals, das in der Regel als Spalte eines Datensatzes vorliegt. Variablen im Kontext von Programmiersprachen beschreiben benannte Referenzen auf bestimmte Datenstrukturen oder Objekte (z.B. numbers = [1, 2, 3]).    Am Beginn jeder statistischen Analyse steht die Aufbereitung und Bereinigung der Daten. Damit ist die Behandlung von fehlenden oder falsch kodierten Werten, die Umkodierung und Transformation von statistischen Variablen oder die Berechnung neuer Spalten gemeint. Oft sind auch nur Untergruppen von Beobachtungen mit bestimmten Merkmausausprägungen von Interesse.\nViele statistische Methoden erfordern auch, dass die Daten nur als numerische Werte vorliegen. Daher müssen ordinale oder nominale Variablen, die als Text gespeichert sind (zum Beispiel ['male', 'female', 'female', ...]) in entsprechende numerische Werte umkodiert werden. Dabei wird jeder Kategorie ein numerischer Wert zugeordnet.\nDas Standard-Paket um mit Datentabellen in Python zu arbeiten, ist pandas. Das folgende Kapitel stellt anhand von vielen praktischen Beispielen zum Nachmachen die grundlegenden Konzepte in pandas vor. Ein Überblick über die Bibliothek und weitere relevante Python-Pakete gibt es hier.\n Laden Sie sich dieses Jupyter Notebook herunter   Speichern Sie das Notebook in Ihrem Projektordner unter ./notebooks ab. Führen Sie die Beispiele im Notebook aus, während Sie die folgende Kurseinheit durcharbeiten.    Ihre Verzeichnisstruktur vom Projektordner sollte jetzt so aussehen:\ndata-librarian-3 ├── data │ └── Library_Usage.csv ├── notebooks │ ├── pandas_introduction.ipynb │ └── tutorial_jupyter.ipynb    What is Pandas? Introduction Video by Giles McMullen ( Untertitel auswählbar)   Weitere Ressourcen  Python Data Analaysis Tutorials Interaktive Python Online-Tutorials auf learnpython.org Einführung in Python auf kaggle Pandas Tutorial auf kaagle Pandas Cheat Sheet  "
},
{
	"uri": "https://bonartm.github.io/data-librarian/basics/reflection/",
	"title": "Reflexion: Daten an Ihrem Arbeitsplatz",
	"tags": [],
	"description": "",
	"content": " Schreiben Sie einen kurzen Text über die Verwendung von Daten und quantitativen Methoden an Ihrem Arbeitsplatz. Denken Sie dabei über folgende Fragen nach:\n Welche Daten sind bei Ihnen vorhanden? Mit welchen Daten arbeiten Sie oder würden Sie gerne arbeiten? Werden statistische Verfahren oder Maschinelles Lernen schon bei Ihnen eingesetzt? Welche Fragen oder Phänomene würden Sie gerne untersuchen? Was fänden Sie spannend herauszufinden?  Teilen Sie Ihren Text mit den anderen KursteilnehmerInnen auf der Kursplattform. Wenn Sie für die Anderen anonym bleiben möchten, können Sie mir auch den Text schicken und ich lade ihn dann hoch.\n  "
},
{
	"uri": "https://bonartm.github.io/data-librarian/descriptive_statistics/visualizations/",
	"title": "Visualisierungen mit Python",
	"tags": [],
	"description": "",
	"content": "   "
},
{
	"uri": "https://bonartm.github.io/data-librarian/descriptive_statistics/visualizations/tutorial/",
	"title": "Tutorial",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://bonartm.github.io/data-librarian/descriptive_statistics/visualizations/examples/",
	"title": "Beispiele",
	"tags": [],
	"description": "",
	"content": "Scatterplots "
},
{
	"uri": "https://bonartm.github.io/data-librarian/basics/quiz_pandas/",
	"title": "Quiz: Pandas-Bibliothek",
	"tags": [],
	"description": "",
	"content": " .quiz fieldset { border-color: black; border-width: 10px; margin-bottom: 1em; } .quiz legend { font-size: 105%; font-weight: 600; padding-left: 15px; padding-right: 15px; padding-top: 15px; } .quiz label { display: block; line-height: 1.75em; } .quiz input[type=\"radio\"] { margin-right: 10px; page-break-after: avoid; page-break-before: avoid; } .quiz input[type=\"submit\"] { background: black; color: white; display: block; font-size: 120%; font-weight: 600; height: 2.5em; margin-top: 2em; text-transform: uppercase; width: 100%; } .quiz table { color: white; font-weight: bold; margin: 1em auto 2em auto; width: 100%; } .quiz td { padding: 5px 15px; text-align: left; width: 60px; } .quiz td.missing-label, .quiz td.missing-score { background: #CECBC2; } .quiz td.right-label, .quiz td.right-score { background: #74b559; } .quiz td.wrong-label, .quiz td.wrong-score { background: #D01F3C; }    var choices = \"read_excel,to_excel,load_excel,from_excel\".split(\",\"); var id = \"pandas_quiz\"; var question = \"Wie lautet der Name der Pandas Funktion, mit der Excel-Dateien eingelesen werden können?\"; var answer = 1 ; if (! (id in questions)){ questions[id] = []; } questions[id].push(new Question(question, choices, answer-1));   var choices = \"df.rows,len(df),df.shape[1],df.size\".split(\",\"); var id = \"pandas_quiz\"; var question = \"Mit welchem Befehl kann die Anzahl der Zeilen aus einem DataFrame auslesen?\"; var answer = 2 ; if (! (id in questions)){ questions[id] = []; } questions[id].push(new Question(question, choices, answer-1));   var choices = \"40,423448,0,215\".split(\",\"); var id = \"pandas_quiz\"; var question = \"Wie viele fehlende Werte enthält die \\x27Age Range\\x27 Variable des Datensatzes?\"; var answer = 4 ; if (! (id in questions)){ questions[id] = []; } questions[id].push(new Question(question, choices, answer-1));   var choices = \"263544,7797,159904,12003\".split(\",\"); var id = \"pandas_quiz\"; var question = \"Wie viele Beobachtungen zwischen 60 und 64 Jahren waren im Jahr 2016 aktive Kunden oder Kundinnen der Bibliothek?\"; var answer = 2 ; if (! (id in questions)){ questions[id] = []; } questions[id].push(new Question(question, choices, answer-1));   var quiz = new Quiz(\"pandas_quiz\", questions, {\"shuffle\": true});   "
},
{
	"uri": "https://bonartm.github.io/data-librarian/descriptive_statistics/quiz_statistics/",
	"title": "Quiz: Deskriptive Statistik",
	"tags": [],
	"description": "",
	"content": " .quiz fieldset { border-color: black; border-width: 10px; margin-bottom: 1em; } .quiz legend { font-size: 105%; font-weight: 600; padding-left: 15px; padding-right: 15px; padding-top: 15px; } .quiz label { display: block; line-height: 1.75em; } .quiz input[type=\"radio\"] { margin-right: 10px; page-break-after: avoid; page-break-before: avoid; } .quiz input[type=\"submit\"] { background: black; color: white; display: block; font-size: 120%; font-weight: 600; height: 2.5em; margin-top: 2em; text-transform: uppercase; width: 100%; } .quiz table { color: white; font-weight: bold; margin: 1em auto 2em auto; width: 100%; } .quiz td { padding: 5px 15px; text-align: left; width: 60px; } .quiz td.missing-label, .quiz td.missing-score { background: #CECBC2; } .quiz td.right-label, .quiz td.right-score { background: #74b559; } .quiz td.wrong-label, .quiz td.wrong-score { background: #D01F3C; }    var quiz = new Quiz(\"pandas_quiz\", questions, {\"shuffle\": true});   "
},
{
	"uri": "https://bonartm.github.io/data-librarian/organisation/packages/pandas/",
	"title": "pandas",
	"tags": [],
	"description": "Bearbeitung, Transformation, Aggregation und Zusammenfassung von Datensätzen. Baut auf numpy auf.",
	"content": "pandas baut auf numpy auf und vereinfacht stark die Bearbeitung, Transformation, Aggregation und Zusammenfassung von zweidimensionalen Datensätzen sowie deren Import und Export in Python. Die zentralen Datenstrukturen in pandas sind Series und DataFrame.\nSeries sind eindimensionale Listen eines Datentypes, ähnlich wie arrays in numpy. Datentypen können ganzzahlige Zahlen (int), binäre Werte vom Typ true oder false (bool), Strings (str) oder reale Zahlen (float) sein.\nIn einem DataFrame werden mehrere Series gleicher Länge spaltenweise zu einer zweidimensionalen Tabelle (wie einer Excel Tabelle) zusammengefasst. Ein DataFrame besitzt außerdem auch immer Spalten- und Zeilennamen.\nWie auch numpy, bietet pandas darüber hinaus viele Funktionen aus der Statistik, zum Beschreiben von Daten. Eine Übersicht gibt es hier.\n# import the library and give it a shorter name \u0026#39;pd\u0026#39; import pandas as pd # create a dataframe by hand with two columns and three rows df = pd.DataFrame({ \u0026#39;month\u0026#39;: [1, 2, 3], \u0026#39;temperatur\u0026#39;: [-12, 3, 9] }) # print out some descriptive statistics df.describe()     Kopieren Sie das Codebeispiel in ein Jupyter Notebook und führen Sie es aus. Fügen Sie weitere Temperatur und Monats-Werte dem DataFrame hinzu. Welche Statistiken liefert ein Aufruf der Funktion describe()?    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/descriptive_statistics/quiz_visualizations/",
	"title": "Quiz: Visualisierungen",
	"tags": [],
	"description": "",
	"content": " .quiz fieldset { border-color: black; border-width: 10px; margin-bottom: 1em; } .quiz legend { font-size: 105%; font-weight: 600; padding-left: 15px; padding-right: 15px; padding-top: 15px; } .quiz label { display: block; line-height: 1.75em; } .quiz input[type=\"radio\"] { margin-right: 10px; page-break-after: avoid; page-break-before: avoid; } .quiz input[type=\"submit\"] { background: black; color: white; display: block; font-size: 120%; font-weight: 600; height: 2.5em; margin-top: 2em; text-transform: uppercase; width: 100%; } .quiz table { color: white; font-weight: bold; margin: 1em auto 2em auto; width: 100%; } .quiz td { padding: 5px 15px; text-align: left; width: 60px; } .quiz td.missing-label, .quiz td.missing-score { background: #CECBC2; } .quiz td.right-label, .quiz td.right-score { background: #74b559; } .quiz td.wrong-label, .quiz td.wrong-score { background: #D01F3C; }    var quiz = new Quiz(\"pandas_quiz\", questions, {\"shuffle\": true});   "
},
{
	"uri": "https://bonartm.github.io/data-librarian/inference/overview/",
	"title": "Überblick",
	"tags": [],
	"description": "",
	"content": "Bisher haben Sie vorliegende Daten einer Stichprobe mit Visualisierungen und Statistiken beschrieben und zusammengefasst. Von Interesse sind aber in der Regel, die Zusammenhänge und Statistiken in der Gesamtpopulation.\nBeispiel Wahlumfrage: Sie ziehen zufällig $n=100$ Personen aus dem Wahlregister und befragen Sie nach ihren Parteipräferenzen. Sie können dann beispielsweise den relativen Anteil der Personen in Ihrer Stichprobe, die eine bestimmte Partei favorisieren, bestimmen. Damit haben Sie einen Schätzwert für den tatsächlichen Wert, wenn Sie alle Personen des Wahlregisters befragt hätten.\nZiehen Sie eine weitere Stichprobe, so werden die neuen Schätzwerte nicht genau mit denen aus der vorherigen Stichprobe übereinstimmen. Wollen Sie deswegen eine Aussage über die tatsächlichen Anteile in der Gesamtpopulation treffen, so ist diese immer mit Unsicherheit behaftet.\nDer Mittelwert oder der relative Anteil ändert sich mit jeder Stichprobe. Damit sind diese Schätzwerte also wieder Variablen, die bestimmten Schwankungen unterliegen und die Sie deskriptiv beschreiben können.\nWie können Sie von Mittelwerten einer Stichprobe auf den \u0026ldquo;wahren\u0026rdquo; Wert in der Gesamtpopulation schließen? Wie können Sie die Unsicherheiten, die dabei auftreten quantifizieren? Mit diesen Fragen beschäftigt sich die Inferenzstatistik.\nBeispiel Würfel: Das würfeln eines Würfels ist ein Zufallsprozess, mit den Ereignissen $\\{1, 2, 3, 4, 5, 6\\}$. Wenn $X$ das Ergebnis eines einzelnen Würfelwurfs beschreibt dann kann die Wahrscheinlichkeit, dass der Würfelwurf Eins ergibt mit $P(X = 1)$ geschrieben werden. Ist der würfel fair und würfeln wir diesen nun viele hunderttausende Mal, so wird der relative Anteil der Einserwürfe $P(X=1) = \\frac{1}{6}$ ergeben.\nWahrscheinlichkeiten nehmen immer Werte zwischen Null und Eins an und können deswegen auch als Prozente zwischen $0%$ und $100%$ angegeben werden.\nJe mehr Beobachtungen gesammelt werden, desto genauer können Aussagen über die Wahrscheinlichkeiten von Zufallsexperimenten getroffen werden. Würfeln wir beispielsweise nur $n=10$ mal und berechnen die relative Anteile, so werden die Ergebnisse noch stark schwanken. Je öfter aber das Experiment wiederholt wird, desto näher wird der gemessene relative Anteil an den wahren Wert $\\frac{1}{6}$ sein.\nStichprobenfehler Der Stichprobenfehler gibt an, wie stark ein Schätzwert (Relativer Anteil Personen mit Präferenz für Partei A) von Stichprobe zu Stichprobe schwankt. Damit wird also die Varianz eines Schätzers angegeben. Die theoretische Bestimmung des Stichprobenfehlers ist eine der Hauptaufgaben der Inferenzstatistik.\nNormalerweise ziehen Sie nur eine Stichprobe. Wie können Sie dann von dieser einen Stichprobe auf die Varianz des Schätzers schließen?\nDer Zentrale Grenzwertsatz ist eine der Hauptaussagen der Inferenzstatistik. Er besagt, dass die Schätzwerte von ausreichend großen Zufallsstichproben einer Normalverteilung mit fest bestimmbaren Mittelwert und Varianz folgen.\nMit dem\nBias Dieser Begriff beschreibt eine systematische Verzerrung des Schätzwertes. Dies kann aufgrund von falschen Berechnungen des Schätzers oder häufiger, aufgrund von schlechten Stichproben passieren.\nBefragen wir beispielsweise nur Personen in Großstädten nach ihren Wahlpräferenzen, so sind die daraus gewonnen Daten nur sehr schlechte Schätzwerte für die wahren Werte der Gesamtpopulation, da bestimmte Präferenzen systematisch über- oder unterschätzt werden.\n  "
},
{
	"uri": "https://bonartm.github.io/data-librarian/organisation/packages/matplotlib/",
	"title": "matplotlib",
	"tags": [],
	"description": "Bietet 2D Plotting Funktionalitäten.",
	"content": "matplotlib ist das Standard-Paket zum Erstellen von wissenschaftlichen 2-dimensionalen statischen Graphiken. Die grundlegende Struktur in matplotlib ist figure, eine leere graphische Fläche, die mit Linien, Balken, Punten, Beschriftungen und Axen befüllt werden kann. Der fertige Plot kann dann in diversen Formaten abgespeichert oder auf dem Bildschirm angezeigt werden.\n# import the package and give it the shorter name \u0026#39;plt\u0026#39; # matplotlib inline import matplotlib.pyplot as plt # create some dummy data x = range(1, 10) # make a simple scatter plot of the data plt.plot(x, x, c=\u0026#34;green\u0026#34;, linestyle=\u0026#39;\u0026#39;, marker=\u0026#39;+\u0026#39;)     Kopieren Sie den Code in ein Jupyter Notebook. Ändern Sie die Farbe der Pukte im Plot von grün auf schwarz. Ändern Sie den Aufruf so um, dass statt Punkte, Linien angezeigt werden. Hier finden Sie die Dokumentation der Funktion matplotlib.pyplot.plot.    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/inference/ttest/",
	"title": "Mittelwertvergleiche",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://bonartm.github.io/data-librarian/organisation/packages/seaborn/",
	"title": "seaborn",
	"tags": [],
	"description": "Verbesserung und Weiterentwicklung der matplotlib Bibliothek.",
	"content": "seaborn baut auf matplotlib auf und bietet eine Vielzahl von Funktionen, die es erlauben schnell und einfach schöne statistische Visualisierungen zu erstellen. Seaborn ist also keine komplett eigenenständige Graphik-Bibliothek, sondern nutzt intern die Funktionalitäten und Datenstrukturen von matplotlib.\nEine wichtige Funktion ist die sns.set() Methode. Wenn sie am Anfang eines Python-Scripts ausgeführt wird, wird intern das Design der Plots erheblich verbessert. Alle plots, die nach dem Aufruf der Funktion erstellt werden, sehen viel besser aus.\nTesten Sie den Unterschied mit dem folgenden Beispiel:\n# import the libraries and give them some shorter names import matplotlib.pyplot as plt import seaborn as sns # setup the seaborn library sns.set() # create the same plot as in the previous example x = range(1, 10) plt.plot(x, x)    Wenn Sie im Jupyter Notebook das Code-Beispiel ausgeführt haben und danach den Aufruf sns.set() entfernen, ändert sich das Design des Plots erstmal nicht. Für einen \u0026ldquo;Reset\u0026rdquo; müssen Sie den Kernel (also der im Hintergrund laufende Python Prozess) mit einem Klick auf neu starten.\n  "
},
{
	"uri": "https://bonartm.github.io/data-librarian/organisation/packages/scipy/",
	"title": "scipy",
	"tags": [],
	"description": "Funktionen und Methoden aus der Statistik.",
	"content": "scipy ist fest mit numpy und pandas verbunden und bietet eine Menge an Funktionen und Methoden aus der Mathematik und Statistik an.\nFür uns ist vor alle das Paket scipy.stats Interessant. Mit ihm können Zufallszahlen aus verschiedensten statistischen Verteilungen generiert werden oder auch statistische Tests durchgeführt werden. Hier finden Sie einen Überblick über alle Methoden des Pakets.\nIm folgenden Beispiel wird ein Zweistichproben-t-Test an zwei numerischen Listen durchgeführt.\n# import the package stats from the library scipy from scipy import stats # create two numerical arrays x = [12, 10, 11, 13, 14, 10, 13, 13, 22] y = [1, 4, 2, 3, 5, 2, 1, 0, 0, 1, 2] # perform a two sample t-test, to test if the samples have different means stats.ttest_ind(x,y)    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/organisation/packages/scikitlearn/",
	"title": "scitkit-learn",
	"tags": [],
	"description": "Bietet Funktionen und Methoden für maschinelles Lernen.",
	"content": "scikit-learn ist eine umfangreiche Bibliothek für maschinelles Lernen in Python. Es bietet eine Vielzahl an verschiedenen Algorithmen, mit denen zum Beispiel Vorhersagen oder Bilderkennung durchgeführt werden können.\n  Faces recognition example using eigenfaces and SVMshttps://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html#sphx-glr-auto-examples-applications-plot-face-recognition-py\n  # import the packages import numpy as np from sklearn.linear_model import LinearRegression # create some dummy dependent and independent variable X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]]) y = - 1 * X[:,0] + 2 * X[:,1] # estimate a linear regression and print out the coefficients reg = LinearRegression().fit(X, y) reg.coef_    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/basics/",
	"title": "Grundlagen",
	"tags": [],
	"description": "",
	"content": "27.01 – 02.02 Grundlagen und deskriptive Statistik Diese Einheit gibt eine Einführung in die Aufgaben und grundlegenden Begriffe der angewandten Statistik. Im zweiten Teil wird das pandas Paket vorgestellt und gezeigt, wie Datensätze eingelesen und bearbeitet werden können.\nZiele  Beschreiben Sie Datensätze mit dem statistischen Grundvokabular Lesen Sie Datensätze als DataFrames in Python ein und aus Filtern Sie DataFrames nach Spalten oder Zeilen Erstellen Sie neue Variablen Projektaufgabe Die Pressestelle der San Francisco Public Library möchte einen Online-Artikel zum Kundenstamm der Bibliothek erstellen. Dazu hat sie Ihnen einen Datensatz geschickt, den Sie auswerten sollen.\n  Erstellen Sie eine Beschreibung des Datensatzes unter Verwendung des statistischen Grundvokabulars\n  Lesen Sie den Datensatz ein und berechnen Sie für jeden Kunden\n die Dauer der Mitgliedschaft in Jahren die Anzahl der Ausleihen und Verlängerungen pro Jahr der Mitgliedschaft    Senden Sie bis Freitag Ihre Beschreibung zusammen mit den Berechnungen in Form eines integrierten Python Notebooks an malte@bonart.de.\n      "
},
{
	"uri": "https://bonartm.github.io/data-librarian/descriptive_statistics/",
	"title": "Deskriptive Statistik und Visualisierungen",
	"tags": [],
	"description": "",
	"content": "03.02 – 09.02 Deskriptive Statistik II und Visualisierungen Dieses Modul gibt eine Einführung in die deskriptive Statistik mit pandas und zeigt, wie statistische Visualisierungen in Python erstellt werden können.\nZiele  Berechnen und interpretieren Sie grundlegende Lage- und Streuungsmaße Beschreiben Sie univariate stetige und diskrete Verteilungen Beschreiben und Berechnen Sie Statistiken für stetige und diskrete bivariate Verteilungen Erstellen Sie einfache Visualisierungen  Projektaufgabe Für den Online-Artikel zum Kundenstamm der Bibliothek braucht die Pressestelle einige interessanten Zahlen zum Thema Alter und Bibliotheksnutzung. Außerdem möchte sie die Daten in einer Info-Graphik zusammenstellen.\nFür eine erste Demo sind Sie verantwortlich.\n Berechnen Sie 2-3 Statistiken und Erstellen Sie 2-3 Visualisierungen basierend auf den Informationen im Datensatz. Nutzen Sie pandas zur Berechnung der Statistiken und seaborn oder matplotlib für die Visualisierungen. Senden Sie bis Freitag Ihren Report in Form eines integrierten Python Notebooks an malte@bonart.de.    Beispielfragen, die Sie mit dem Datensatz beantworten und visualisieren können:\n Wie viele Senioren und Kinder sind Kunden der San Francisco Public Library? Wie viele Nutzer möchten per Mail informiert werden? Wie alt sind diese Nutzer durchschnittlich im Vergleich zu Nutzern, die per Post informiert werden möchten? Wie viele Ausleihen werden im Mittel pro Altersgruppe und pro Jahr getätigt? Ist die Streuung zwischen den Gruppen gleich?    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/inference/",
	"title": "Inferenzstatistik",
	"tags": [],
	"description": "",
	"content": "10.02 – 16.02 Grundlagen der Inferenzstatistik Diese Einheit gibt einen Einblick in die Inferenzstatistik und stellt die Berechnung von Hypothesentest in Python vor.\nZiele  Führen Sie einen Hypothesentest für den Vergleich von zwei Mittelwerten durch.  Projektaufgabe Unterscheidet sich das Ausleihverhalten von jungen und älteren Bibliotheksnutzern signifikant voneinander?\n Erstellen Sie jeweils eine Liste der Total Checkouts für YOUNG ADULT und SENIOR aus der Spalte Patron Type Defintion. Wie viele Ausleihen pro Person haben die beiden Nutzergruppen jeweils im Mittel getätigt? Führen Sie einen Mittelwerttest durch. Verwenden Sie dafür die Funktion ttes_ind aus dem scipy.stats Paket Senden Sie bis Freitag Ihren Report in Form eines integrierten Python Notebooks an [malte@bonart.de] (mailto:malte@bonart.de).    "
},
{
	"uri": "https://bonartm.github.io/data-librarian/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://bonartm.github.io/data-librarian/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]